{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functools\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_pass(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kw):\n",
    "        time_begin = time.time()\n",
    "        result = func(*args, **kw)\n",
    "        time_stop = time.time()\n",
    "        time_passed = time_stop - time_begin\n",
    "        minutes, seconds = divmod(time_passed, 60)\n",
    "        hours, minutes = divmod(minutes, 60)\n",
    "        print('%s: %s:%s:%s' % (func.__name__, int(hours), int(minutes), int(seconds)))\n",
    "        return result\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@time_pass\n",
    "def read_big_csv(path):\n",
    "    reader = pd.read_csv(path, chunksize=10000)\n",
    "    data = pd.concat(reader, axis=0, ignore_index=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def cal_dup_ratio(series, n):\n",
    "    \"\"\"\n",
    "    计算一个人给另一个人不同月份转的频次\n",
    "    \"\"\"\n",
    "    the_dup_ratio = np.zeros(4)\n",
    "    tmp = pd.Series(series.value_counts().values).value_counts()\n",
    "    for j in tmp.index:\n",
    "        if j > 3:\n",
    "            continue\n",
    "        else:\n",
    "            the_dup_ratio[j] = tmp[j] / n\n",
    "\n",
    "    the_dup_ratio[0] = 1 - np.sum(the_dup_ratio)\n",
    "\n",
    "    return the_dup_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_both(id_loc):\n",
    "    \"\"\"\n",
    "    data_from: 取值为dat_edge_from\n",
    "    data_to: 取值为dat_edge_to\n",
    "    \"\"\"\n",
    "    the_id_dat_from = one_step_son[int(id_loc[1]):int(id_loc[2])][['to_id', 'times_sum', 'weight_sum']].copy()\n",
    "    the_id_dat_to = one_step_father[int(id_loc[3]):int(id_loc[4])][['from_id', 'times_sum', 'weight_sum']].copy()\n",
    "\n",
    "    if the_id_dat_from['times_sum'].dtype == 'object':\n",
    "        the_id_dat_from['times_sum'] = list(map(eval, the_id_dat_from['times_sum']))\n",
    "\n",
    "    if the_id_dat_to['times_sum'].dtype == 'object':\n",
    "        the_id_dat_to['times_sum'] = list(map(eval, the_id_dat_to['times_sum']))\n",
    "\n",
    "    agg_the_id_dat_from = the_id_dat_from.groupby('to_id')['times_sum', 'weight_sum'].sum()\n",
    "    agg_the_id_dat_to = the_id_dat_to.groupby('from_id')['times_sum', 'weight_sum'].sum()\n",
    "\n",
    "    agg_dat = pd.concat([agg_the_id_dat_from, agg_the_id_dat_to], axis=0)\n",
    "\n",
    "    value_counts_pre = the_id_dat_from['to_id'].append(the_id_dat_to['from_id'])\n",
    "    if len(value_counts_pre) > 0:\n",
    "        both_dup_ratio = cal_dup_ratio(value_counts_pre, len(agg_dat))\n",
    "        both_dup_ratio_3 = both_dup_ratio[3]\n",
    "    else:\n",
    "        both_dup_ratio_3 = np.nan\n",
    "    \n",
    "    if len(the_id_dat_to['from_id']) > 0:\n",
    "        in_dup_ratio = cal_dup_ratio(the_id_dat_to['from_id'], len(agg_the_id_dat_to))\n",
    "        in_dup_ratio_3 = in_dup_ratio[3]\n",
    "    else:\n",
    "        in_dup_ratio_3 = np.nan\n",
    "    \n",
    "    # 开始计算特征\n",
    "    length = len(value_counts_pre)\n",
    "    unique_count = len(agg_dat)\n",
    "    if length > 0:\n",
    "        multi_ratio = (length - unique_count) / length\n",
    "    else:\n",
    "        multi_ratio = np.nan\n",
    "\n",
    "    weight = agg_dat['weight_sum']\n",
    "    if len(weight) > 0:\n",
    "        weight_0 = np.min(weight)\n",
    "        weight_25 = np.percentile(weight, 25)\n",
    "        weight_50 = np.percentile(weight, 50)\n",
    "        weight_100 = np.max(weight)\n",
    "    else:\n",
    "        weight_0, weight_25, weight_50, weight_100 = np.nan, np.nan, np.nan, np.nan\n",
    "    \n",
    "    times = agg_dat['times_sum']\n",
    "    value_counts = times.value_counts()\n",
    "    value_counts_0_9 = value_counts.reindex(list(range(10)))\n",
    "    value_counts_0_9[0] = len(times) - np.nansum(value_counts_0_9)\n",
    "    value_counts_ratio = value_counts_0_9/len(times)\n",
    "    \n",
    "    if len(agg_the_id_dat_to) > 0:\n",
    "        in_weight_0 = np.min(agg_the_id_dat_to['weight_sum'])\n",
    "        in_weight_25 = np.percentile(agg_the_id_dat_to['weight_sum'], 25)\n",
    "        in_weight_50 = np.percentile(agg_the_id_dat_to['weight_sum'], 50)\n",
    "        in_weight_100 = np.max(agg_the_id_dat_to['weight_sum'])\n",
    "    else:\n",
    "        in_weight_0, in_weight_25, in_weight_50, in_weight_100 = np.nan, np.nan, np.nan, np.nan\n",
    "        \n",
    "    if len(agg_the_id_dat_from) > 0:\n",
    "        out_weight_0 = np.min(agg_the_id_dat_from['weight_sum'])\n",
    "        out_weight_50 = np.percentile(agg_the_id_dat_from['weight_sum'], 50)\n",
    "        out_weight_100 = np.max(agg_the_id_dat_from['weight_sum'])\n",
    "    else:\n",
    "        out_weight_0, out_weight_50, out_weight_100 = np.nan, np.nan, np.nan\n",
    "    \n",
    "    columns = (['both_dup_ratio_3', 'in_dup_ratio_3', 'length', 'unique_count', 'multi_ratio'] \n",
    "               + ['weight_0', 'weight_25', 'weight_50', 'weight_100'] \n",
    "               + ['value_counts_ratio_%s' % str(x) for x in range(10)]\n",
    "               + ['in_weight_0', 'in_weight_25', 'in_weight_50', 'in_weight_100'] \n",
    "               + ['out_weight_0', 'out_weight_50', 'out_weight_100'])\n",
    "    \n",
    "    result = ([both_dup_ratio_3, in_dup_ratio_3, length, unique_count, multi_ratio] \n",
    "              + [weight_0, weight_25, weight_50, weight_100] \n",
    "              + list(value_counts_ratio) \n",
    "              + [in_weight_0, in_weight_25, in_weight_50, in_weight_100] \n",
    "              + [out_weight_0, out_weight_50, out_weight_100])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_big_csv: 0:0:18\n",
      "read_big_csv: 0:0:16\n",
      "read_big_csv: 0:0:20\n",
      "read_big_csv: 0:0:20\n",
      "read_big_csv: 0:0:17\n",
      "read_big_csv: 0:0:17\n",
      "read_big_csv: 0:0:17\n",
      "read_big_csv: 0:0:17\n",
      "read_big_csv: 0:0:17\n",
      "read_big_csv: 0:0:17\n",
      "read_big_csv: 0:0:1\n"
     ]
    }
   ],
   "source": [
    "input_path = './'\n",
    "sample_train = pd.read_table(os.path.join(input_path, \"open_data/sample_train.txt\"))  # 训练集约1.9万\n",
    "valid_id = pd.read_table(os.path.join(input_path, \"open_data/valid_id.txt\"))  # 验证集\n",
    "test_id = pd.read_table(os.path.join(input_path, \"open_data/test_id.txt\"))  # 测试集\n",
    "\n",
    "file_names = ['dat_edge_feature_%s.csv' % str(x) for x in range(1, 12)]\n",
    "dat_edge_feature = reduce(lambda x, y: x.append(y),\n",
    "                          (read_big_csv('./output/dat_edge_feature/%s' % z) for z in file_names))\n",
    "\n",
    "son = pd.read_csv('./output/son.csv')\n",
    "father = pd.read_csv('./output/father.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 43s, sys: 1min 14s, total: 3min 58s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "one_step = pd.DataFrame({'id': list(set(son['to_id']).union(set(father['from_id'])))})\n",
    "\n",
    "one_step_son = pd.merge(one_step, dat_edge_feature, left_on='id', right_on='from_id')\n",
    "one_step_father = pd.merge(one_step, dat_edge_feature, left_on='id', right_on='to_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(dat_edge_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.6 s, sys: 42 s, total: 1min 2s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "one_step_son.sort_values(by='id', inplace=True)\n",
    "one_step_son.reset_index(drop=True, inplace=True)\n",
    "one_step_father.sort_values(by='id', inplace=True)\n",
    "one_step_father.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.41 s, sys: 1.4 s, total: 5.8 s\n",
      "Wall time: 5.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "one_step_son_id_counts = one_step_son['id'].value_counts(sort=False).sort_index().cumsum()\n",
    "one_step_father_id_counts = one_step_father['id'].value_counts(sort=False).sort_index().cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 32.8 s, sys: 88 ms, total: 32.9 s\n",
      "Wall time: 32.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "id_loc_info_from = pd.DataFrame({'id': list(one_step_son_id_counts.index),\n",
    "                                 'start_from': [0] + list(one_step_son_id_counts.values)[:-1],\n",
    "                                 'stop_from': list(one_step_son_id_counts.values)})\n",
    "id_loc_info_to = pd.DataFrame({'id': list(one_step_father_id_counts.index),\n",
    "                               'start_to': [0] + list(one_step_father_id_counts.values)[:-1],\n",
    "                               'stop_to': list(one_step_father_id_counts.values)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.57 s, sys: 0 ns, total: 1.57 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "id_loc_info = pd.merge(id_loc_info_from, id_loc_info_to, on='id', how='outer')\n",
    "id_loc_info = id_loc_info[['id', 'start_from', 'stop_from', 'start_to', 'stop_to']]\n",
    "id_loc_info.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_loc_s = list(id_loc_info.values)  # 生成一个list,准备并行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 9.2 s, total: 1min 18s\n",
      "Wall time: 53min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with Pool(20) as p:\n",
    "    feature_7 = p.map(cal_both, id_loc_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 388 ms, total: 1min 8s\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "columns = (['both_dup_ratio_3', 'in_dup_ratio_3', 'length', 'unique_count', 'multi_ratio'] \n",
    "           + ['weight_0', 'weight_25', 'weight_50', 'weight_100'] \n",
    "           + ['value_counts_ratio_%s' % str(x) for x in range(10)]\n",
    "           + ['in_weight_0', 'in_weight_25', 'in_weight_50', 'in_weight_100'] \n",
    "           + ['out_weight_0', 'out_weight_50', 'out_weight_100'])\n",
    "\n",
    "feature_7_df = pd.DataFrame(feature_7, columns=columns)\n",
    "feature_7_df['id'] = id_loc_info['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_7_df.to_csv('./output/feature_7_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
